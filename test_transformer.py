import os
from unittest import main
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from transformer import *

np.random.seed(0)

def is_allclose(x, y):
    return np.allclose(x, y, rtol=1.e-3, atol=1.e-5)

def test_MultiHeadAttention():
    m = MultiHeadAttention(d_model=8)
    # [batch size, length, d_model]
    batch_size, length, d_model = 2, 5, 8
    in_t = torch.rand((batch_size, length, d_model))
    mask = torch.tensor([[1, 1, 0, 0, 0], [1, 1, 1, 0, 0]], dtype=torch.bool)
    out_t = m(in_t, mask)
    
    expected_out = [[[1.5807247161865234, 0.45447567105293274, -1.7011611461639404, 1.0619759559631348, 0.11123042553663254, -0.7106012105941772, 0.06887102127075195, -0.8655168414115906], [1.2535359859466553, 1.3708804845809937, 0.12983737885951996, 0.476717472076416, -1.3599733114242554, 0.005264593288302422, -1.52761971950531, -0.34864261746406555], [-1.4434890747070312, 1.201569676399231, 0.18403059244155884, 0.48643389344215393, -1.0385146141052246, 1.6140427589416504, -0.5847786068916321, -0.41929465532302856], [0.36241933703422546, 1.0511846542358398, -1.4815928936004639, 0.8493155241012573, -0.4072973132133484, 0.9669477343559265, -1.6314139366149902, 0.29043683409690857], [-0.9636484384536743, -0.24737098813056946, -1.1628546714782715, 0.6123365163803101, -0.40970465540885925, -0.8108600378036499, 1.5602607727050781, 1.4218415021896362]], [[-2.033726215362549, 1.0053622722625732, -0.7727583050727844, 0.12376683950424194, 0.5445009469985962, 0.8160191178321838, 0.9416278600692749, -0.6247929334640503], [-0.6155330538749695, 1.2035956382751465, -1.6831568479537964, 0.19799090921878815, 0.781985878944397, -0.6612893342971802, -0.5991047024726868, 1.3755114078521729], [-0.6071768999099731, 0.8691067099571228, -1.412085771560669, 0.0006177976028993726, -1.477426290512085, 1.0506125688552856, 1.2073396444320679, 0.3690118193626404], [-1.4863152503967285, 1.2873743772506714, -1.1031792163848877, -0.3522786498069763, 0.7952829599380493, 1.342639684677124, -0.5877726674079895, 0.10424862802028656], [-0.42654356360435486, -0.4153544306755066, -1.2207903861999512, -1.5047979354858398, 0.9262778759002686, 1.513411521911621, 0.3341263234615326, 0.793670654296875]]]
    
    return is_allclose(out_t.detach().numpy(), np.array(expected_out))

def test_PositionwiseFeedForward():
    batch_size, length, d_model, d_ff = 2, 5, 8, 4
    p = PositionwiseFeedForward(d_model=d_model, d_ff=d_ff)
    in_t = torch.rand((batch_size, length, d_model))
    out_t = p(in_t)
    expected_out = [[[-0.44843754172325134, 0.29755887389183044, 0.8898659944534302, 1.2813653945922852, 0.9720780253410339, -0.09426024556159973, -1.6971361637115479, -1.2010343074798584], [0.3877794146537781, 1.0232164859771729, -0.314208060503006, 1.9903984069824219, -0.4457288086414337, -0.6062679290771484, -0.7885335683822632, -1.2466552257537842], [-0.006127507891505957, 0.91069096326828, 0.18744157254695892, 1.9204696416854858, 0.08128701895475388, -0.6290241479873657, -1.171150803565979, -1.2935867309570312], [0.8860822916030884, -0.006118941120803356, -1.0892117023468018, 2.185899019241333, -0.3096735179424286, -0.8072893619537354, -0.6877645254135132, -0.17192219197750092], [0.27522605657577515, 0.5740747451782227, -0.8953801393508911, 1.551703929901123, 1.2995657920837402, -0.9889744520187378, -1.0937118530273438, -0.7225033640861511]], [[-0.15403907001018524, 0.45634180307388306, -0.7871341109275818, 1.709700107574463, 0.939591109752655, 0.20099659264087677, -0.6805245280265808, -1.6849323511123657], [-0.768927812576294, 0.5739848017692566, 0.6538254618644714, 1.1976871490478516, 1.2859941720962524, -1.433227300643921, -0.32438817620277405, -1.1849480867385864], [-0.4453659653663635, 1.0224138498306274, -1.2314249277114868, 1.2527005672454834, 1.0489903688430786, -0.5571167469024658, -1.4576705694198608, 0.367473840713501], [-1.0161141157150269, 0.8483806252479553, -0.15367840230464935, 1.893255591392517, 0.6739885210990906, -1.1273103952407837, -0.939422607421875, -0.17909951508045197], [0.9450676441192627, -0.6425668001174927, -0.5485059022903442, 1.2312116622924805, 1.5595849752426147, -0.48428013920783997, -1.2393591403961182, -0.8211520910263062]]]
    return is_allclose(out_t.detach().numpy(), np.array(expected_out))

def test_TransformerEncoder():
    batch_size, length, d_model, d_ff = 2, 5, 8, 4
    in_t = torch.rand((batch_size, length, d_model))
    mask = torch.tensor([[1, 1, 0, 0, 0], [1, 1, 1, 0, 0]], dtype=torch.bool)

    tr = TransformerEncoder(d_model=d_model, d_ff=d_ff)
    
    out_t = tr(in_t, mask)
    expected_out = [[[1.0716462135314941, -1.8465886116027832, 0.1143297404050827, -0.6234487295150757, -1.0559117794036865, 0.847495436668396, 0.5306291580200195, 0.9618485569953918], [-1.0952773094177246, -1.671668529510498, 0.3471405506134033, 1.2956619262695312, -0.24255551397800446, 0.7238933444023132, -0.5203766226768494, 1.163182258605957], [-0.1557377278804779, -1.6361894607543945, 0.20587891340255737, 1.861363172531128, 0.5314754247665405, 0.36844927072525024, -0.0033477647230029106, -1.1718920469284058], [-1.1897895336151123, -1.1918801069259644, -0.24623055756092072, 1.1262518167495728, -0.25516098737716675, 1.9299955368041992, -0.20857428014278412, 0.03538822755217552], [-0.01683635078370571, -0.6831976771354675, -0.25511434674263, -0.8205751776695251, 0.5996361970901489, 2.3163352012634277, -1.0280917882919312, -0.1121559739112854]], [[-1.2621105909347534, -1.7711633443832397, 1.0301042795181274, 0.9577032327651978, -0.5865125060081482, 0.7321171760559082, 0.39131617546081543, 0.5085455775260925], [-0.6245882511138916, 1.1259005069732666, -1.548123836517334, -0.3494691550731659, -0.16042383015155792, 1.9135503768920898, -0.3685999810695648, 0.01175406202673912], [0.2201566994190216, -2.234992265701294, 0.3875183165073395, 1.4593945741653442, 0.6104852557182312, 0.2395065575838089, -0.4237004518508911, -0.25836876034736633], [-0.42254313826560974, -1.7748427391052246, -0.572269082069397, 0.9218260049819946, 0.36509382724761963, 1.163720965385437, 1.148371696472168, -0.8293575048446655], [0.2666677236557007, -1.026247262954712, -0.6084670424461365, 2.3054590225219727, 0.39487165212631226, -0.5271486043930054, 0.06240152567625046, -0.8675370812416077]]]
    
    return is_allclose(out_t.detach().numpy(), np.array(expected_out))

if __name__ == '__main__':
    # import sys
    # main(*sys.argv[1:])
    mha = test_MultiHeadAttention()
    pff = test_PositionwiseFeedForward()
    te = test_TransformerEncoder()
    assert mha is True
    assert pff is True
    assert te is True
    print(mha, pff, te)